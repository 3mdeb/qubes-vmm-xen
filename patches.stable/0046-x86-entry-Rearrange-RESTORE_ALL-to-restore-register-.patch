From f7e273a07ccf993063727675589f10da206f1683 Mon Sep 17 00:00:00 2001
From: Andrew Cooper <andrew.cooper3@citrix.com>
Date: Wed, 14 Feb 2018 12:37:33 +0100
Subject: x86/entry: Rearrange RESTORE_ALL to restore register
 in stack order

Results in a more predictable (i.e. linear) memory access pattern.

No functional change.

This is part of XSA-254.

Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
Reviewed-by: Jan Beulich <jbeulich@suse.com>
Reviewed-by: Wei Liu <wei.liu2@citrix.com>
master commit: f85d105e27735f0e20aa30d77f03774f3ed55ae5
master date: 2018-01-05 19:57:08 +0000
---
 xen/include/asm-x86/asm_defns.h | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/xen/include/asm-x86/asm_defns.h b/xen/include/asm-x86/asm_defns.h
index f05209b533..54328e7b53 100644
--- a/xen/include/asm-x86/asm_defns.h
+++ b/xen/include/asm-x86/asm_defns.h
@@ -256,7 +256,6 @@ static always_inline void stac(void)
  * @compat: R8-R15 don't need reloading
  */
 .macro RESTORE_ALL adj=0 compat=0
-        LOAD_C_CLOBBERED \compat
 .if !\compat
         movq  UREGS_r15(%rsp),%r15
         movq  UREGS_r14(%rsp),%r14
@@ -265,6 +264,7 @@ static always_inline void stac(void)
 .endif
         movq  UREGS_rbp(%rsp),%rbp
         movq  UREGS_rbx(%rsp),%rbx
+        LOAD_C_CLOBBERED \compat
         subq  $-(UREGS_error_code-UREGS_r15+\adj), %rsp
 .endm
 
-- 
2.13.6

